{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# import ipdb\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelcaptions = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "df = pd.read_csv('./all-data.csv', header=None, sep=\",\", encoding='ISO-8859-1', names=[\"label\",\"text\"])\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: labelcaptions[x])\n",
    "df[\"sentence_id\"] = np.array(list(range(len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>$AEZS BOOM !!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Discord https://t.co/VJCTdWxjI6  To get advise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$TGTX few hours negative action making people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>$gaxy $spy $vvpr $nio $xspa $opti $fb $idex $i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Today Top Flow in S&amp;amp;P 500 #SP500, Buy Flow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11994</th>\n",
       "      <td>11994</td>\n",
       "      <td>Trimmed $UPWK ðŸ’¯% gain (still holding 50% of or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>11995</td>\n",
       "      <td>$SPG whereâ€™s the guys braving that they sold a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>11996</td>\n",
       "      <td>$TRHC Results of Tabula Rasa HealthCareâ€™s Firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>11997</td>\n",
       "      <td>$Mist is looking good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>11998</td>\n",
       "      <td>$SNAP just blast up or down ffs. Just MOVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11999 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               text\n",
       "0          0                                      $AEZS BOOM !!\n",
       "1          1  Discord https://t.co/VJCTdWxjI6  To get advise...\n",
       "2          2  $TGTX few hours negative action making people ...\n",
       "3          3  $gaxy $spy $vvpr $nio $xspa $opti $fb $idex $i...\n",
       "4          4  Today Top Flow in S&amp;P 500 #SP500, Buy Flow...\n",
       "...      ...                                                ...\n",
       "11994  11994  Trimmed $UPWK ðŸ’¯% gain (still holding 50% of or...\n",
       "11995  11995  $SPG whereâ€™s the guys braving that they sold a...\n",
       "11996  11996  $TRHC Results of Tabula Rasa HealthCareâ€™s Firs...\n",
       "11997  11997                              $Mist is looking good\n",
       "11998  11998         $SNAP just blast up or down ffs. Just MOVE\n",
       "\n",
       "[11999 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./private-test-set.csv', sep=\",\", header=0, encoding='UTF-8', names=[\"ID\",\"text\"])[:-1]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERT parameters\n",
    "bert_tokenizer_model_id = 'bert-base-uncased'\n",
    "bert_pretrained_model_id = 'google/bert_uncased_L-12_H-768_A-12'\n",
    "\n",
    "## other training parameters\n",
    "max_doc_length = 256   # max in train data is 62 in main and 258 in extra data\n",
    "clip = 0.25            #gradient clipping\n",
    "lr = 0.00003           #initial learning rate\n",
    "wdecay=1.2e-6          #weight decay applied to all weights\n",
    "epochs = 30            #maximum number of epochs\n",
    "batch_size = 64         #batch size\n",
    "save = 'model.pt'      #path to save the final model\n",
    "use_extra_data = True  #if extra data should be used\n",
    "\n",
    "train_max_number_batches = -1 # only for the sake of debugging. Set to -1 to be ignored\n",
    "inference_max_number_batches = -1 # only for the sake of debugging. Set to -1 to be ignored\n",
    "\n",
    "## log parameters\n",
    "log_interval = 100     #log interval during training\n",
    "log_interval_val = 100 #log interval during validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data items: torch.Size([4846, 150]) (4846,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_model_id, do_lower_case=True)\n",
    "\n",
    "def convert_text_bertids_tensor(texts): return tokenizer(texts, padding=True, return_tensors=\"pt\", truncation=True, max_length=max_doc_length)\n",
    "\n",
    "def get_document_label_tensor(df, test=False):\n",
    "    documents = []\n",
    "    for x in df['text'].values:\n",
    "        documents.append(x.strip())\n",
    "    data = convert_text_bertids_tensor(documents)\n",
    "    \n",
    "    if test:\n",
    "        ID = df['ID'].values.astype(np.int64)\n",
    "        return ID, data\n",
    "    else:\n",
    "        labels = np.array(list(df['label'].values))\n",
    "        return data, labels\n",
    "\n",
    "x, y = get_document_label_tensor(df)\n",
    "print ('Data items:', str(x['input_ids'].shape), str(y.shape))\n",
    "\n",
    "# dataloaders \n",
    "dataset = TensorDataset(x['input_ids'], x['token_type_ids'], x['attention_mask'], torch.LongTensor(y))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=7, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST\n",
    "ID_test, x_test = get_document_label_tensor(df_test, test=True)\n",
    "dataset_test = TensorDataset(x_test['input_ids'], x_test['token_type_ids'], x_test['attention_mask'], torch.LongTensor(ID_test))\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=7, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'models/' + 'BERT_valacc=86.54%_bs=6_doc_len=256_epoch=1_lr=3e-05_valsize=20_bert_uncased_L-12_H-768_A-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch:\n",
      "1 GPU(s) available.\n",
      "GPU-Name: A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch GPU capabilities:\n",
    "\n",
    "print(\"\\nPyTorch:\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('%d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('GPU-Name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifierModel(torch.nn.Module):\n",
    "    '''\n",
    "    Classification model with BERT\n",
    "    '''\n",
    "    def __init__(self, bert, nout):\n",
    "        super(BERTClassifierModel, self).__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.embedding_size = self.bert.config.hidden_size \n",
    "\n",
    "        self.output_projection_layer = torch.nn.Linear(self.embedding_size, nout)\n",
    "\n",
    "    '''\n",
    "    input format: seq_len, batch\n",
    "    '''\n",
    "    def forward(self, input_batch):       \n",
    "        #### complete the code (2 points) - START ####\n",
    "        # use the contents of `input_batch`, and pass them as parameters to self.bert\n",
    "        # use the output of BERT together with self.output_projection_layer to provide predictions \n",
    "        # the final results should consist of two variables:\n",
    "        #   `log_probs` -> tensor of logarithms of the predicted probabilities for classes \n",
    "        #   `final_representations` -> tensor of the output BERT vectors, based on which the prediction is done (for visualization purposes) \n",
    "        \n",
    "        _out = self.bert.forward(input_ids=input_batch[\"input_ids\"], attention_mask=input_batch[\"attention_mask\"], token_type_ids=input_batch[\"token_type_ids\"])\n",
    "        \n",
    "        final_representations = _out[\"last_hidden_state\"][:,0,:]\n",
    "        logits = self.output_projection_layer(final_representations)\n",
    "        #ipdb.set_trace()\n",
    "        \n",
    "        log_probs = torch.nn.LogSoftmax(dim=1)(logits)\n",
    "\n",
    "        #### complete the code - END ####\n",
    "        \n",
    "        return log_probs, final_representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BERTClassifierModel(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (output_projection_layer): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained(bert_pretrained_model_id, cache_dir=\"cache\", output_attentions=True)\n",
    "\n",
    "model = BERTClassifierModel(bert=bert, nout=len(labelcaptions.keys()))\n",
    "model.to(device)\n",
    "print('Model:', model)\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "params = list(model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=wdecay)\n",
    "\n",
    "stored_res = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(fn):\n",
    "    with open(fn, 'rb') as f:\n",
    "        model_state, criterion_state, optimizer_state = torch.load(f)\n",
    "    return model_state, criterion_state, optimizer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X): return [np.exp(x) / np.sum(np.exp(x)) for x in X]\n",
    "\n",
    "def predict(dataloader, model):\n",
    "    \n",
    "    model.to(device)\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    _predictions = []\n",
    "    _outputs = []\n",
    "    _labels = []\n",
    "    _representations = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        _input_ids, _token_type_ids, _attention_mask, _label = [e.to(device) for e in batch]\n",
    "        _input_batch = {'input_ids': _input_ids,\n",
    "                        'token_type_ids': _token_type_ids, \n",
    "                        'attention_mask': _attention_mask}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _output, _batch_representations = model.forward(_input_batch)\n",
    "            _batch_predictions = torch.argmax(_output, dim=1)\n",
    "        _outputs.extend(softmax(_output.cpu().numpy()))\n",
    "        _predictions.extend(_batch_predictions.cpu().numpy())\n",
    "        _labels.extend(_label.cpu().numpy())\n",
    "        _representations.extend(_batch_representations.cpu().numpy())\n",
    "        \n",
    "        if i % log_interval_val == 0 and i > 0:\n",
    "            print('Prediction | %5d batches | %5d data |' % (i, i*batch_size))\n",
    "            \n",
    "        if (i > inference_max_number_batches) and (inference_max_number_batches != -1):\n",
    "            break\n",
    "            \n",
    "    return _predictions, _outputs, _labels, np.array(_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       596\n",
      "           1       0.95      0.92      0.93      2854\n",
      "           2       0.88      0.89      0.88      1350\n",
      "\n",
      "    accuracy                           0.91      4800\n",
      "   macro avg       0.89      0.92      0.90      4800\n",
      "weighted avg       0.92      0.91      0.91      4800\n",
      "\n",
      "=========================================================================================\n",
      "| End of testing | test accuracy 0.914 \n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "EVAL_MEASURE = 'accuracy'\n",
    "\n",
    "# Load the best saved model.\n",
    "model_state, criterion_state, optimizer_state = model_load(path)\n",
    "model.load_state_dict(model_state)\n",
    "criterion.load_state_dict(criterion_state)\n",
    "optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "# Run on test data.\n",
    "yhat, yhat_proba, y, x_test_representations = predict(dataloader, model)\n",
    "results = classification_report(y, yhat, output_dict=True)\n",
    "print (classification_report(y, yhat))    \n",
    "\n",
    "print('=' * 89)\n",
    "print('| End of testing | test %s %.3f ' % (EVAL_MEASURE, results[EVAL_MEASURE]))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### TEST ##################\n",
    "def predict_test(dataloader, model):\n",
    "    \n",
    "    model.to(device)\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    _predictions = []\n",
    "    _outputs = []\n",
    "    _labels = []\n",
    "    _representations = []\n",
    "    IDS = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        _input_ids, _token_type_ids, _attention_mask, ID = [e.to(device) for e in batch]\n",
    "        _input_batch = {'input_ids': _input_ids,\n",
    "                        'token_type_ids': _token_type_ids, \n",
    "                        'attention_mask': _attention_mask}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _output, _batch_representations = model.forward(_input_batch)\n",
    "            _batch_predictions = torch.argmax(_output, dim=1)\n",
    "        _outputs.extend(softmax(_output.cpu().numpy()))\n",
    "        _predictions.extend(_batch_predictions.cpu().numpy())\n",
    "        _representations.extend(_batch_representations.cpu().numpy())\n",
    "        IDS.extend(ID.cpu().numpy())\n",
    "        \n",
    "        if i % log_interval_val == 0 and i > 0:\n",
    "            print('Prediction | %5d batches | %5d data |' % (i, i*batch_size))\n",
    "            \n",
    "        if (i > inference_max_number_batches) and (inference_max_number_batches != -1):\n",
    "            break\n",
    "            \n",
    "    return IDS, _predictions, _outputs, np.array(_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction |   100 batches |  6400 data |\n"
     ]
    }
   ],
   "source": [
    "############# TEST ###############################\n",
    "IDS, yhat, yhat_proba, x_test_representations = predict_test(dataloader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### TEST #######################\n",
    "import csv\n",
    "def save_results(yhat, IDS):\n",
    "    preds = [{val:key for key,val in labelcaptions.items()}[e] for e in yhat]\n",
    "    df = pd.DataFrame({'ID': IDS, 'Text': preds})\n",
    "    df.to_csv(\"submission_test1.csv\", index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11994</th>\n",
       "      <td>11994</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>11995</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>11996</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>11997</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>11998</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11999 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID      Text\n",
       "0          0  positive\n",
       "1          1   neutral\n",
       "2          2  positive\n",
       "3          3  positive\n",
       "4          4  positive\n",
       "...      ...       ...\n",
       "11994  11994  positive\n",
       "11995  11995  negative\n",
       "11996  11996   neutral\n",
       "11997  11997  positive\n",
       "11998  11998   neutral\n",
       "\n",
       "[11999 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = save_results(yhat, IDS)\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

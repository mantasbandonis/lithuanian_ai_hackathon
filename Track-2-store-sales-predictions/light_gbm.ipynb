{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train = pd.read_csv('train_data.csv', parse_dates=['Date'])\n",
    "test = pd.read_csv('test_data.csv', parse_dates=['Date'])\n",
    "sample_sub = pd.read_csv('test_predictions_format.csv')\n",
    "print('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['StoreType']\n",
    "del train['SchoolHoliday']\n",
    "del train['StateHoliday']\n",
    "del train['DayOfWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['StoreType']\n",
    "del test['SchoolHoliday']\n",
    "del test['StateHoliday']\n",
    "del test['DayOfWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train_or_test'] = 'train'\n",
    "test['train_or_test'] = 'test'\n",
    "df = pd.concat([train,test], sort=False)\n",
    "print('Combined df shape:{}'.format(df.shape))\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting date features\n",
    "df['dayofmonth'] = df.Date.dt.day\n",
    "df['dayofyear'] = df.Date.dt.dayofyear\n",
    "df['dayofweek'] = df.Date.dt.dayofweek\n",
    "df['month'] = df.Date.dt.month\n",
    "df['year'] = df.Date.dt.year\n",
    "df['weekofyear'] = df.Date.dt.weekofyear\n",
    "df['is_month_start'] = (df.Date.dt.is_month_start).astype(int)\n",
    "df['is_month_end'] = (df.Date.dt.is_month_end).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Store','AssortmentType','Date'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sales_agg_monthwise_features(df, gpby_cols, target_col, agg_funcs):\n",
    "    '''\n",
    "    Creates various sales agg features with given agg functions  \n",
    "    '''\n",
    "    gpby = df.groupby(gpby_cols)\n",
    "    newdf = df[gpby_cols].drop_duplicates().reset_index(drop=True)\n",
    "    for agg_name, agg_func in agg_funcs.items():\n",
    "        aggdf = gpby[target_col].agg(agg_func).reset_index()\n",
    "        aggdf.rename(columns={target_col:target_col+'_'+agg_name}, inplace=True)\n",
    "        newdf = newdf.merge(aggdf, on=gpby_cols, how='left')\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sales lag features\n",
    "def create_sales_lag_feats(df, gpby_cols, target_col, lags):\n",
    "    gpby = df.groupby(gpby_cols)\n",
    "    for i in lags:\n",
    "        df['_'.join([target_col, 'lag', str(i)])] = \\\n",
    "                gpby[target_col].shift(i).values + np.random.normal(scale=1.6, size=(len(df),))\n",
    "    return df\n",
    "\n",
    "# Creating sales rolling mean features\n",
    "def create_sales_rmean_feats(df, gpby_cols, target_col, windows, min_periods=2, \n",
    "                             shift=1, win_type=None):\n",
    "    gpby = df.groupby(gpby_cols)\n",
    "    for w in windows:\n",
    "        df['_'.join([target_col, 'rmean', str(w)])] = \\\n",
    "            gpby[target_col].shift(shift).rolling(window=w, \n",
    "                                                  min_periods=min_periods,\n",
    "                                                  win_type=win_type).mean().values +\\\n",
    "            np.random.normal(scale=1.6, size=(len(df),))\n",
    "    return df\n",
    "\n",
    "# Creating sales rolling median features\n",
    "def create_sales_rmed_feats(df, gpby_cols, target_col, windows, min_periods=2, \n",
    "                            shift=1, win_type=None):\n",
    "    gpby = df.groupby(gpby_cols)\n",
    "    for w in windows:\n",
    "        df['_'.join([target_col, 'rmed', str(w)])] = \\\n",
    "            gpby[target_col].shift(shift).rolling(window=w, \n",
    "                                                  min_periods=min_periods,\n",
    "                                                  win_type=win_type).median().values +\\\n",
    "            np.random.normal(scale=1.6, size=(len(df),))\n",
    "    return df\n",
    "\n",
    "# Creating sales exponentially weighted mean features\n",
    "def create_sales_ewm_feats(df, gpby_cols, target_col, alpha=[0.9], shift=[1]):\n",
    "    gpby = df.groupby(gpby_cols)\n",
    "    for a in alpha:\n",
    "        for s in shift:\n",
    "            df['_'.join([target_col, 'lag', str(s), 'ewm', str(a)])] = \\\n",
    "                gpby[target_col].shift(s).ewm(alpha=a).mean().values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, ohe_cols=['Store','AssortmentType','dayofmonth','dayofweek','month','weekofyear']):\n",
    "    '''\n",
    "    One-Hot Encoder function\n",
    "    '''\n",
    "    print('Creating OHE features..\\nOld df shape:{}'.format(df.shape))\n",
    "    df = pd.get_dummies(df, columns=ohe_cols)\n",
    "    print('New df shape:{}'.format(df.shape))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting sales to log(1+sales)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "df['Sales'] = df['Sales'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_series = (df.year==2015) & (df.month.isin([1,2,3,4,5,6,7]))\n",
    "masked_series2 = (df.year==2015) & (~(df.month.isin([1,2,3,4,5,6,7])))\n",
    "df.loc[(masked_series), 'train_or_test'] = 'val'\n",
    "df.loc[(masked_series2), 'train_or_test'] = 'test'\n",
    "print('Train shape: {}'.format(df.loc[df.train_or_test=='train',:].shape))\n",
    "print('Validation shape: {}'.format(df.loc[df.train_or_test=='val',:].shape))\n",
    "print('No train shape: {}'.format(df.loc[df.train_or_test=='no_train',:].shape))\n",
    "print('Test shape: {}'.format(df.loc[df.train_or_test=='test',:].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting sales of validation period to nan so as to resemble test period\n",
    "train = df.loc[df.train_or_test.isin(['train','val']), :]\n",
    "Y_val = train.loc[train.train_or_test=='val', 'Sales'].values.reshape((-1))\n",
    "Y_train = train.loc[train.train_or_test=='train', 'Sales'].values.reshape((-1))\n",
    "train.loc[train.train_or_test=='val', 'Sales'] = np.nan\n",
    "\n",
    "# # Creating sales lag, rolling mean, rolling median, ohe features of the above train set\n",
    "train = create_sales_lag_feats(train, gpby_cols=['Store', 'AssortmentType'], target_col='Sales', \n",
    "                               lags=[91,98,105,112,119,126,182,364,546,728])\n",
    "\n",
    "train = create_sales_rmean_feats(train, gpby_cols=['Store'], \n",
    "                                 target_col='Sales', windows=[364,546], \n",
    "                                 min_periods=10, win_type='triang') #98,119,91,182,\n",
    "\n",
    "# # train = create_sales_rmed_feats(train, gpby_cols=['store','item'], \n",
    "# #                                 target_col='sales', windows=[364,546], \n",
    "# #                                 min_periods=10, win_type=None) #98,119,91,182,\n",
    "\n",
    "train = create_sales_ewm_feats(train, gpby_cols=['Store','AssortmentType'], \n",
    "                               target_col='Sales', \n",
    "                               alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5], \n",
    "                               shift=[91,98,105,112,119,126,182,364,546,728])\n",
    "\n",
    "# # Creating sales monthwise aggregated values\n",
    "# agg_df = create_sales_agg_monthwise_features(df.loc[df.train_or_test=='train', :], \n",
    "#                                              gpby_cols=['store','item','month'], \n",
    "#                                              target_col='sales', \n",
    "#                                              agg_funcs={'mean':np.mean, \n",
    "#                                              'median':np.median, 'max':np.max, \n",
    "#                                              'min':np.min, 'std':np.std})\n",
    "\n",
    "# # Joining agg_df with train\n",
    "# train = train.merge(agg_df, on=['store','item','month'], how='left')\n",
    "\n",
    "# One-Hot Encoding \n",
    "train = one_hot_encoder(train, ohe_cols=['Store','AssortmentType','dayofweek','month']) \n",
    "#,'dayofmonth','weekofyear'\n",
    "\n",
    "# Final train and val datasets\n",
    "val = train.loc[train.train_or_test=='val', :]\n",
    "train = train.loc[train.train_or_test=='train', :]\n",
    "print('Train shape:{}, Val shape:{}'.format(train.shape, val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "avoid_cols = ['Date', 'Sales', 'train_or_test', 'Id', 'year']\n",
    "cols = [col for col in train.columns if col not in avoid_cols]\n",
    "print('No of training features: {} \\nAnd they are:{}'.format(len(cols), cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(preds, target):\n",
    "    '''\n",
    "    Function to calculate SMAPE\n",
    "    '''\n",
    "    n = len(preds)\n",
    "    masked_arr = ~((preds==0)&(target==0))\n",
    "    preds, target = preds[masked_arr], target[masked_arr]\n",
    "    num = np.abs(preds-target)\n",
    "    denom = np.abs(preds)+np.abs(target)\n",
    "    smape_val = (200*np.sum(num/denom))/n\n",
    "    return smape_val\n",
    "\n",
    "def lgbm_smape(preds, train_data):\n",
    "    '''\n",
    "    Custom Evaluation Function for LGBM\n",
    "    '''\n",
    "    labels = train_data.get_label()\n",
    "    smape_val = smape(np.expm1(preds), np.expm1(labels))\n",
    "    return 'SMAPE', smape_val, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'task':'train', 'boosting_type':'gbdt', 'objective':'regression', \n",
    "              'metric': {'rmse'}, 'num_leaves': 10, 'learning_rate': 0.02, \n",
    "              'feature_fraction': 0.8, 'max_depth': 5, 'verbose': 0, \n",
    "              'num_boost_round':3000, 'early_stopping_rounds':None, 'nthread':-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbtrain = lgb.Dataset(data=train.loc[:,cols].values, label=Y_train, \n",
    "                       feature_name=cols)\n",
    "lgbval = lgb.Dataset(data=val.loc[:,cols].values, label=Y_val, \n",
    "                     reference=lgbtrain, feature_name=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_validation(params, lgbtrain, lgbval, X_val, Y_val, verbose_eval):\n",
    "    t0 = time.time()\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgbtrain, num_boost_round=params['num_boost_round'], \n",
    "                      valid_sets=[lgbtrain, lgbval], feval=lgbm_smape, \n",
    "                      early_stopping_rounds=params['early_stopping_rounds'], \n",
    "                      evals_result=evals_result, verbose_eval=verbose_eval)\n",
    "    print(model.best_iteration)\n",
    "    print('Total time taken to build the model: ', (time.time()-t0)/60, 'minutes!!')\n",
    "    pred_Y_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    pred_Y_val = np.expm1(pred_Y_val)\n",
    "    Y_val = np.expm1(Y_val)\n",
    "    val_df = pd.DataFrame(columns=['true_Y_val','pred_Y_val'])\n",
    "    val_df['pred_Y_val'] = pred_Y_val\n",
    "    val_df['true_Y_val'] = Y_val\n",
    "    print(val_df.shape)\n",
    "    print(val_df.sample(5))\n",
    "    print('SMAPE for validation data is:{}'.format(smape(pred_Y_val, Y_val)))\n",
    "    return model, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training lightgbm model and validating\n",
    "model, val_df = lgb_validation(lgb_params, lgbtrain, lgbval, val.loc[:,cols].values, \n",
    "                               Y_val, verbose_eval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see top 25 features as identified by the lightgbm model.\n",
    "print(\"Features importance...\")\n",
    "gain = model.feature_importance('gain')\n",
    "feat_imp = pd.DataFrame({'feature':model.feature_name(), \n",
    "                         'split':model.feature_importance('split'), \n",
    "                         'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "print('Top 25 features:\\n', feat_imp.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whole = create_sales_lag_feats(df, gpby_cols=['Store','AssortmentType'], target_col='Sales', \n",
    "                                  lags=[91,98,105,112,119,126,182,364,546,728])\n",
    "df_whole = create_sales_rmean_feats(df_whole, gpby_cols=['Store','AssortmentType'], \n",
    "                                    target_col='Sales', windows=[364,546], \n",
    "                                    min_periods=10, win_type='triang')\n",
    "# df = create_sales_rmed_feats(df, gpby_cols=['store','item'], target_col='sales', \n",
    "#                              windows=[364,546], min_periods=2) #98,119,\n",
    "df_whole = create_sales_ewm_feats(df_whole, gpby_cols=['Store','AssortmentType'], target_col='Sales', \n",
    "                                  alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5], \n",
    "                                  shift=[91,98,105,112,119,126,182,364,546,728])\n",
    "\n",
    "# # Creating sales monthwise aggregated values\n",
    "# agg_df = create_sales_agg_monthwise_features(df.loc[~(df.train_or_test=='test'), :], \n",
    "#                                              gpby_cols=['store','item','month'], \n",
    "#                                              target_col='sales', \n",
    "#                                              agg_funcs={'mean':np.mean, \n",
    "#                                              'median':np.median, 'max':np.max, \n",
    "#                                              'min':np.min, 'std':np.std})\n",
    "\n",
    "# # Joining agg_df with df\n",
    "# df = df.merge(agg_df, on=['store','item','month'], how='left')\n",
    "\n",
    "# One-Hot Encoding\n",
    "df_whole = one_hot_encoder(df_whole, ohe_cols=['Store','AssortmentType','dayofweek','month']) \n",
    "#'dayofmonth',,'weekofyear'\n",
    "\n",
    "# Final train and test datasets\n",
    "test = df_whole.loc[df_whole.train_or_test=='test', :]\n",
    "train = df_whole.loc[~(df_whole.train_or_test=='test'), :]\n",
    "print('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM dataset\n",
    "lgbtrain_all = lgb.Dataset(data=train.loc[:,cols].values, \n",
    "                           label=train.loc[:,'Sales'].values.reshape((-1,)), \n",
    "                           feature_name=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train(params, lgbtrain_all, X_test, num_round):\n",
    "    t0 = time.time()\n",
    "    model = lgb.train(params, lgbtrain_all, num_boost_round=num_round, feval=lgbm_smape)\n",
    "    test_preds = model.predict(X_test, num_iteration=num_round)\n",
    "    print('Total time taken in model training: ', (time.time()-t0)/60, 'minutes!')\n",
    "    return model, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training lgb model on whole data(train+val)\n",
    "lgb_model, test_preds = lgb_train(lgb_params, lgbtrain_all, test.loc[:,cols].values, model.best_iteration)\n",
    "print('test_preds shape:{}'.format(test_preds.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "sub = test.loc[:,['Id','Sales']]\n",
    "sub['Sales'] = test_preds\n",
    "sub['Id'] = sub.Id.astype(int)\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-authority",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
